Upscale (StormTrade) — Senior Golang Developer  
Январь 2023 — по настоящее время, ~2 года 10 месяцев

В Upscale я работаю Senior Golang-разработчиком в продуктовой финтех-компании, которая делает проп-трейдинг платформу для трейдеров. Суть в том, что трейдер покупает доступ к аккаунту с крупным капиталом, проходит 2 этапа торговли, в каждом из которых нужно поторговать 5-7 дней в плюс, причем в конце каждого этапа баланс сбрасывается. И вот потом он переходит в статус Funded и тогда мы ему доверяем тот номинал, который он купил. И он уже выводит 80% прибыли

### Как была устроена команда и процессы

Команда:  
Я вхожу в кросс-функциональную продуктовую команду: 6 Go-бэкендеров (включая меня), 2 фронтенд-разработчика, 2 QA, продакт-менеджер, бизнес-аналитик, плюс отдельно платформенная/DevOps-команда. Команда частично распределённая, работаем в удалённом формате.

Процессы и методология:

- Работали по Scrum c двухнедельными спринтами. В начале спринта — планирование: продакт приносит задачи от бизнеса, мы их уточняем, декомпозируем, оцениваем по сторипойнтам.
- Обязательные дейли минут по 15 — что сделал, что планирую, какие блокеры.
- Раз в неделю — груминг бэклога: разбираем крупные идеи, чтобы к планированию они уже были с понятными критериями готовности.
- В конце спринта — демо для стейкхолдеров и ретроспектива, где обсуждаем как продуктовые, так и процессные вещи.

Инструменты:  
Задачи ведём в Plane, документацию — в Confluence, для письменной коммуникации — Telegram, а для устной — Gather. Каждый merge-request обязательно проходит код-ревью минимум у одного коллеги, для сложных изменений — у двух. Плюс у нас есть регулярные 1:1 с тимлидом — обсуждаем фокус, развитие и архитектурные решения.

Отдельный пласт — работа с продом:  
У нас есть on-call-ротация среди разработчиков. Я периодически дежурю по продакшену: слежу за алертами (Prometheus + Alertmanager), участвую в разборе инцидентов и написании коротких postmortemов.

---

### Продукт и моя зона ответственности

По продукту есть несколько крупных доменов:

- торговое ядро (ордеры, позиции, расчёт PnL и маржи),
- аккаунты трейдеров и риск-менеджмент,
- выплаты, реферальная система, промокоды и маркетинговая инфраструктура,
- аналитика торговли и сервисы «подсказок» трейдерам.

Я больше всего завязан на торговое ядро и риск-менеджмент, но параллельно веду часть сервисов вокруг клиентской инфраструктуры и аналитики.

### Торговое ядро и миграция с монолита на микросервисы

Когда я пришёл, ядро было по сути крупным монолитом на Go: в одном сервисе жили и приём ордеров, и риск-проверки, и управление аккаунтами. С ростом до тысяч активных трейдеров и миллионов ордеров стало видно, что монолит не масштабируется — p95 задержки по ордерам росли, а любое изменение затрагивало половину системы.

Что мы сделали:

- Вместе с тимлидом и ещё одним сеньором мы разделили домены: выделили отдельные сервисы для приёма ордеров (order-gateway), управления аккаунтами и балансами, риск-движка, расчёта PnL, сервиса котировок и payouts-контур.
- В качестве транспорта между микросервисами используем gRPC для синхронных запросов и Kafka для событий (создание/закрытие ордера, изменение статуса аккаунта, пересчёт метрик риска и т.д.).
- Для критичного к задержкам функционала (матчинг ордеров, пересчёт маржи) держим состояние in-memory: на каждом инстансе поднимаются воркеры, которые держат в памяти карту аккаунтов и позиций. Периодически пишем снапшоты в PostgreSQL и дублируем часть оперативных данных в Redis, чтобы в приёмниках событий не ходить лишний раз в основную БД.

Технически это выглядело так:

- Для каждого аккаунта у нас есть «шард», за который отвечает отдельный воркер — он получает события через каналы (ордеры, тики цен, изменения по аккаунту), последовательно применяет их к in-memory состоянию и отправляет команды на запись.
- Для устойчивости мы предусмотрели механизм восстановления: при рестарте сервиса шард сначала поднимает состояние из последнего снапшота в PostgreSQL, а затем догоняет хвост по событиям из Kafka.

Результат:  
После поэтапного разрезания монолита и выноса торговой логики в набор микросервисов мы снизили p95 задержку обработки ордеров примерно с 250–300 мс до 90–120 мс при нагрузке до 4–5K rps на критичных сервисах. При этом за счёт in-memory подхода и грамотного батчинга записи в БД мы в пиках уменьшили нагрузку на PostgreSQL примерно в 2–3 раза.

---

### Обработка котировок и оптимизация хранилищ

Отдельный кусок работы был связан с котировками и данными:

- Изначально котировки приходили отдельными сообщениями для каждой пары инструмент/таймфрейм, а дальше полсистемы гоняло эти тикеты по очередям и хранилищам. При всплеске волатильности Kafka-топики распухали, а потребители не успевали.
- Я участвовал в переработке схемы: мы сгруппировали котировки в батчи по инструменту и тайм-слотам, переработали структуру топиков (в том числе добавили отдельные топики под агрегированные значения, которые нужны только аналитике), и перенесли часть дешёвой агрегации прямо в потребителя.

По БД:

- Мы внедрили шардирование данных по идентификаторам аккаунтов и партиционирование по датам, а также аккуратно пересобрали индексы — отдельные для запросов по активным позициям и отдельные для отчётности.
- При проектировании схемы опирались на реальные паттерны запросов: что чаще всего запрашивает UI, что нужно риск-движку, как выглядят отчёты и выгрузки.

За счёт этого мы добились того, что даже при росте числа активных аккаунтов и ордеров время ответов по основным запросам (активные позиции, история сделок за последние N дней) осталось в пределах сотен миллисекунд, а не секунд.

---

### Риск-менеджмент, лимиты и бизнес-логика проп-аккаунтов

Второй большой блок — риск-менеджмент:

- Для каждого аккаунта есть набор правил: дневные и общий drawdown, ограничения по объёму сделок, запрет торговли в периодах повышенной волатильности, правила по удержанию позиций через ночь и т.п.
- Мы реализовали на Go достаточно гибкий «rule engine»: каждый риск-чек реализует интерфейс с методом `Validate(ctx, state, order)`, а сами правила конфигурируются через админку и хранятся в PostgreSQL. При поступлении нового ордера или изменении цены по инструменту мы прогоняем его через цепочку правил, и уже на основе результата даём добро/отклоняем/закрываем позиции.

С точки зрения реализации:

- Много работы было с конкурентным доступом к состоянию — у нас на один аккаунт может приходить сразу несколько событий, поэтому мы строили модель с последовательной обработкой и чёткой сериализацией событий, а не пытались всё параллелить.
- Использовали context с таймаутами и отменой, чтобы любые потенциально долгие операции (например, внешние запросы) не блокировали критичный к задержкам путь.

### Клиентская инфраструктура: уведомления, рефералка, промокоды, почта и сертификаты

Помимо ядра я веду несколько сервисов, которые крутятся вокруг «обёртки» для трейдера.

**Уведомления**  
Мы вынесли уведомления в отдельный сервис:

- На вход он получает события из RabbitMQ (сделка создана/закрыта, достигнут новый рекорд по PnL, нарушен риск-лимит и т.п.).
- На выходе — отправляет email, пуши, внутренняя лента событий в личном кабинете.
- Чтобы не плодить дубликаты, ввели идемпотентные обработчики: у каждого события есть ключ идемпотентности, мы храним его в Redis и не шлём повторно, если событие уже обработано.

**Реферальная система и аналитика**  
Я занимался разработкой реферального контура:

- Храним структуру реферальной сети и события (регистрация, покупка аккаунта, переход на новый этап) в PostgreSQL.
- Для аналитики по источникам трафика и LTV подключили ClickHouse: сделали ETL-сервис, который аггрегирует события и складывает в аналитические таблицы.
- В результате маркетинг и продакт получили удобные отчёты по конверсии рефералов по этапам, окупаемости акций и т.д.

**Промокоды**  
Спроектировал и внедрил систему промокодов:

- Есть разные типы: на бесплатный аккаунт, на скидку, на продление.
- Важный момент — учёт ограничений по количеству активаций и срокам действия. Тут тоже активно использовали идемпотентность, чтобы при повторных попытках активации не раздавать лишние бонусы.

**Сертификаты и почтовый сервис**  
Кроме того, я делал сервис генерации сертификатов успешной торговли (PDF/изображения на основе шаблонов), которые автоматически выдаются после прохождения этапа.  
Плюс мы вынесли интеграцию с почтовыми провайдерами в отдельный почтовый микросервис:

- Поддерживаем несколько внешних провайдеров, есть fallback и ретраи через DLQ в RabbitMQ.
- Это существенно снизило число «битых» отправок и разгрузило остальную часть системы от сетевой логики.

---

### Аналитика торговли и ИИ-сервис подсказок для трейдеров

Ещё один важный блок — аналитика:

- Мы построили сервис, который агрегирует торговую историю в ClickHouse: сделки, дневные просадки, кривую equity.
- С точки зрения реализации это ETL-поток: события о сделках и изменениях по аккаунтам читаем из Kafka, приводим к нормализованному виду и батчами пишем в ClickHouse, чтобы не убивать его мелкими вставками.

На базе этих данных мы сделали сервис анализа стратегии:

- На стороне Go-сервиса мы делаем feature extraction: частота сделок, средний риск на сделку, поведение после серии убытков, удержание позиций перед новостями и т.д.
- Дальше поверх этого набор эвристик и простых ML-моделей (этим занимался data-science, а мы интегрировали) выдаёт рекомендации: где трейдер регулярно превышает риск, где слишком агрессивно усредняется и т.п.

Чтобы не перемалывать одни и те же события, мы активно используем Redis для идемпотентности и кеширования промежуточных результатов.

---

### Интересная задача, которая запомнилась

Одна из задач, которая прям запомнилась, — внедрение онлайн-блокировки торговли по достижении дневного лимита просадки.

Проблема была в том, что:

- Лимит считается на основании внутридневного PnL, а данные о сделках и котировках приходят потоком.
- Нельзя было опираться только на БД — там всё с лагом, плюс мы сильно бы просели по задержкам.

Мы спроектировали решение так:

- В торговом ядре у каждого аккаунта есть in-memory структура с текущей equity и накопленным дневным результатом.
- При каждом исполнении ордера и при существенном изменении цены по открытой позиции мы пересчитываем PnL, и если дневной лимит нарушен — сразу же помечаем аккаунт как «заблокирован» для новых сделок и инициируем закрытие открытых позиций.
- Чтобы не перегружать систему, пересчёт делали не на каждом тикете, а по порогам изменения цены и батчами по нескольким позициям.

Интерес в том, что здесь сильно замешана конкуренция: параллельно могли приходить несколько событий, и мы ловили граничные состояния. Пришлось аккуратно выстроить сериализацию событий, добавить кучу юнит- и интеграционных тестов, прогнать симуляции на исторических данных. В итоге система работает предсказуемо и даёт нам жёсткий контроль по риску.

---

### Про фейл, который многому научил

Был показательный фейл с оптимизацией записи состояний аккаунтов.

Мы хотели уменьшить нагрузку на PostgreSQL и увеличили интервал между флашами in-memory состояний в БД. На тестах всё выглядело нормально, но в проде проявился эффект:

- UI для части пользователей начал показывать слегка устаревшие данные по equity и PnL — задержка была несколько десятков секунд.
- В большинстве случаев это было не критично, но для тех, кто в момент просмотра принимал решение по закрытию позиции, это было неприятно.

Мы быстро заметили аномалии по метрикам и жалобам в поддержку, откатили изменение и дальше уже более аккуратно к нему подошли:

- Разделили интервал флаша по доменам: критичные для UI поля пишем чаще, агрегаты для отчётности — реже.
- Добавили отдельные дистрибутивные тесты, где симулируем «нервного трейдера», который активно торгует и одновременно смотрит на графики.
- Ввели фичефлаги для подобных оптимизаций, чтобы сначала катить на небольшой процент пользователей/аккаунтов.

Этот случай хорошо показал, что в финтех-продукте важно смотреть не только на среднюю нагрузку и p95, но и на пользовательское восприятие свежести данных.

---

### В двух словах про роль

По факту моя роль в Upscale — это не только написание кода, но и:

- участие в проектировании новых микросервисов и потоков данных;
- регулярные код-ревью, менторство более младших ребят;
- участие в on-call ротации и разборе инцидентов;
- совместная работа с продактом и аналитиками — от идеи фичи до продакшен-запуска и мониторинга."




## Яндекс — Golang Backend Developer (Яндекс 360, Яндекс.Мессенджер)

Июль 2020 — Декабрь 2022, Москва

«В Яндексе я работал Golang-бэкенд-разработчиком в команде корпоративного мессенджера, который входил в пакет Яндекс 360 для бизнеса. Это B2B SaaS-платформа: чаты, каналы, интеграции с календарём, почтой, документами — всё, чем пользуются внутри компании, но поверх экосистемы Яндекса.

---

### Команда и процессы

**Команда**

Я был в продуктовой команде мессенджера:

- несколько Go-бэкенд-разработчиков,
- несколько фронтендеров,
- QA-команда,
- SRE-ребята, которые помогали с инфраструктурой,
- продакт-менеджер и аналитик.

Мы работали как кросс-функциональная команда: у нас были и люди, которые понимают домен (коммуникации в компаниях, безопасность), и те, кто отвечает за инфраструктуру и эксплуатацию.

**Методология**

Работали по классике:

- двухнедельные спринты;
- планирование: продакт приносит задачи, мы уточняем, разбиваем, оцениваем;
- ежедневные короткие стендапы — кто что делает, какие блокеры;
- в конце спринта — демо и ретроспектива.

Задачи трекали во внутреннем трекере, у каждого сервиса были свои компоненты/борды. Код-ревью — строго обязательно: без аппрува MR просто не проходил в основную ветку.

**On-call и эксплуатация**

У команды был свой on-call по ключевым сервисам мессенджера:

- я регулярно участвовал в ротации — следил за алертами, разбирался в инцидентах;
- участвовал в подготовке и обновлении runbook’ов: что делать, если упал какой-то конкретный сервис, что проверять в первую очередь;
- после серьёзных инцидентов помогал в postmortem-разборах: фиксировали первопричину и дорожку улучшений.

---

### Общий контекст по продукту и архитектуре

Яндекс.Мессенджер в Яндекс 360 — это **многотенантный B2B-продукт**:

- каждый клиент — это отдельная организация со своими пользователями, правами, политиками;
- к этому же аккаунту привязаны почта, календарь, документы и другие сервисы.

Архитектура на backend’е — набор микросервисов:

- ядро сообщений и диалогов;
- сервисы профилей пользователей и органструктуры;
- сервисы уведомлений;
- интеграции с календарём и документами;
- аналитические сервисы.

Когда я пришёл, часть логики уже была вынесена в микросервисы, часть представляла собой довольно тяжёлые сервисы со «смешанной» ответственностью. Одной из целей команды было **привести архитектуру к более чистому разделению по доменам** и подготовить систему к росту нагрузки.

---

## 1. Ядро мессенджера: сообщения и диалоги

Моя основная зона ответственности — **сервис диалогов и сообщений**. Это сердце мессенджера:

- создание чатов и каналов,
- хранение истории сообщений,
- статусы прочтения,
- списки диалогов у пользователя.

### Проблемы, с которыми мы столкнулись

Когда я пришёл, были несколько болей:

1. **История сообщений в больших чатах открывалась заметно дольше**, чем хотелось бы.
2. **Схема БД росла органично** — много индексов «по факту», не всегда под реальные запросы.
3. Архитектурно некоторые части сервиса начали «раздуваться» — к ним подтягивали всё подряд: и участники, и настройки, и поисковые флаги.

### Что мы делали

#### Пересборка схемы данных и оптимизация запросов

Мы с командой прошлись по основным паттернам запросов:

- как фронт запрашивает историю (по диалогу, по дате, по направлениям — вверх/вниз);
- какие фильтры применяются (служебные сообщения, системные, по типу контента);
- какие ещё сервисы лезут в эти же таблицы.

На основании этого:

- Выделили **отдельные сущности**: диалог, участник диалога, сообщение, курсоры прочтения.
- Для тяжёлых запросов к истории диалога сделали **денормализованную таблицу**, где хранили «плоскую» ленту сообщений с заранее подготовленными колонками под популярные фильтры.
- Пересобрали индексы под реальные запросы, а не «на всякий случай».

Параллельно внедрили **разделение по tenant’ам и временным диапазонам** (партиционирование):

- это позволило не гонять полноразмерные таблицы, когда пользователь открывает недавнюю историю;
- операционные запросы (последние сообщения, недавние диалоги) стали работать быстрее, а старые данные не мешали.

#### Кэширование и работа с Redis

Я активно участвовал в проектировании кэш-слоя:

- Кэшировали **метаданные диалогов** (название, тип, список участников, последний месседж) и **счётчики непрочитанных**.
    
- Особое внимание уделяли **консистентности**:
    
    - при изменении диалога или его участников обязательно инвалидация кэша;
    - для критичных данных использовали короткий TTL и fallback на БД;
    - следили за тем, чтобы при сбоях кэша система продолжала работать, просто чуть медленнее.

### Результат

На выходе:

- стало **проще масштабировать сервис диалогов**,
- время ответа на типовые операции (открыть список диалогов, подгрузить историю) стало стабильнее,
- многие инциденты, связанные с «подлагиваниями» при всплесках активности, удалось снять за счёт правильных индексов и кэширования.

---

## 2. Уведомления и интеграции с экосистемой Яндекс 360

Вторая крупная зона, где я плотно участвовал — **система уведомлений и интеграции с другими сервисами Яндекс 360**.

### Система уведомлений

Цель — чтобы:

- пользователи **оперативно получали уведомления** (мобильные пуши, web-уведомления, иногда письма);
- бизнес мог **настраивать политики** уведомлений под себя;
- система выдерживала нагрузку и не засыпала людей лишними сигналами.

**Как это было устроено на backend’е:**

1. Сервис диалогов и сообщений при событии (новое сообщение, упоминание, приглашение в канал, изменение прав) генерирует **событие**.
    
2. События идут в **Kafka**, откуда их читает сервис уведомлений.
    
3. Сервис уведомлений:
    
    - смотрит настройки пользователя и организации (по каким событиям, на какие устройства, в какое время можно уведомлять);
    - решает, какие каналы задействовать: мобильный пуш, web-уведомление, иногда email;
    - формирует задания для каналов и кладёт их уже в **очереди задач** (RabbitMQ или аналог), откуда спец-воркеры общаются с внешними сервисами.

Моя роль:

- разрабатывал часть логики сервиса уведомлений на Go;
    
- участвовал в проектировании **модели настроек** (что может включать/выключать пользователь, что — администратор компании);
    
- внедрял **идемпотентную обработку событий**, чтобы при ретраях и временных сбоях мы не слали дубли.
    

### Интеграции с календарём и документами

Мессенджер был тесно интегрирован с:

- календарём (чаты вокруг встреч, уведомления о начале/изменении встречи);
    
- документами (обсуждение файлов в чатах, предпросмотр, быстрые ссылки).
    

**На backend-уровне это выглядело так:**

- Определённые события в календаре или документах генерили события (через Kafka или внутренние шины).
    
- Наши сервисы подписывались на них, создавали или обновляли чаты, добавляли служебные сообщения в диалоги.
    
- В обратную сторону — по действиям в мессенджере могли, например, поменять статус участия в встрече.
    

Важно было **правильно строить авторизацию**:

- нельзя просто взять и показать ссылку на документ всем участникам чата;
    
- перед тем, как отдать пользователю предпросмотр или ссылку, backend сверяет его права через общие сервисы аутентификации/авторизации в Яндекс 360.
    

Моя роль:

- я участвовал в проектировании и реализации gRPC/REST-контрактов между мессенджером и соседними сервисами;
    
- помогал «приземлять» требования безопасности на конкретную реализацию: какие токены где проверяем, какие атрибуты передаём, что кэшируем, что — нет.
    

---

## 3. Аналитика и отчётность для B2B-клиентов

Крупные корпоративные заказчики хотели **отчёты по использованию мессенджера**:

- активность пользователей и подразделений;
    
- нагрузка по времени;
    
- какие команды используют мессенджер как основной канал, а где он ещё не прижился.
    

### Как мы строили аналитику

На backend-уровне это выглядело так:

1. Сервис мессенджера генерит события: отправка сообщения, прочтение, логин, смена устройства, присоединение к каналу и так далее.
    
2. Эти события уходят в **Kafka**.
    
3. Отдельный аналитический сервис (в котором я тоже участвовал) читает события, **нормализует** их и пишет в аналитические хранилища — ClickHouse и YT/Ytsaurus.
    

При реализации было несколько задач:

- **Обогащение данных**: к каждому событию подтягивать информацию об организации, подразделении, типе клиента.
    
- **Оптимизация вставок**: писать в аналитическое хранилище батчами, чтобы не перегружать базы постоянными мелкими запросами.
    
- **Стабильность потока**: если downstream временно недоступен, не терять события, а аккуратно накапливать и догонять.
    

Моя роль:

- реализовывал часть ETL-логики на Go: чтение из Kafka, обработка, запись в ClickHouse;
    
- участвовал в проектировании схем таблиц: какие индексы, какие агрегаты хранить заранее, а какие считать на лету;
    
- помогал продуктовым и аккаунт-командам собирать дашборды (на базе этих таблиц) в Grafana или внутренних инструментах.
    

Итог:

- администраторы компаний-клиентов могли **в админке быстро получать отчёты**: как используются чаты, какие команды активны, как меняется использование продукта по времени.
    

---

## 4. Интересная техническая задача

Одна из задач, которая хорошо запомнилась — **оптимизация механизма непрочитанных сообщений в групповых чатах**.

### Как было «до»

Исторически механизм был устроен довольно прямолинейно:

- У каждого пользователя хранится курсор прочтения по диалогу (до какого сообщения он дочитал).
- При заходе в чат система смотрит на курсор, делает запрос в БД и считает, какие сообщения после этого курсора считаются непрочитанными.

Это нормально работает для небольших чатов, но:

- в больших чатах, особенно с активной перепиской, такие запросы становились тяжёлыми;
- счётчики непрочитанных могли считать каждый раз «с нуля», что билось и по БД, и по latency.

### Что мы сделали

Мы предложили переработать схему:

- В БД хранить **базовый курсор** по диалогу (условно, «минимум прочтённого» среди участников) и **разницу по пользователям**.
- В Redis держать **горячие курсоры** и **счётчики непрочитанных** для активных пользователей / диалогов.

При этом логика стала такой:

- при новом сообщении мы инкрементим счётчики в кэше, не лезя каждый раз в БД;
- при отметке «прочитал» — обновляем курсор пользователя и корректируем счётчики;
- при промахе в кэше или сложной ситуации (редкие сценарии) падаем обратно на оптимизированный запрос к БД.

Моя роль:

- участвовал в проектировании новой модели;
- реализовывал часть логики на Go (обновление курсоров, синхронизацию с Redis, fallback-сценарии);
- писал тесты на граничные случаи (массовое добавление пользователей, удаление из чатов, одновременные обновления).

В результате:

- мы сильно снизили нагрузку на БД по этим запросам;
- сделали поведение счётчиков более предсказуемым под высокой нагрузкой.

---

## 5. Фейл, который дал хороший опыт

Был показательный эпизод с **политиками хранения сообщений (retention)** для корпоративных клиентов.

### Контекст

Некоторые клиенты хотели:

- автоматически удалять сообщения старше определённого срока в отдельных чатах или типах каналов;
- иметь разные политики для разных подразделений или типов переписки.

Мы реализовывали backend-часть:

- задания, которые периодически проходятся по данным и помечают сообщения на удаление;
- учёт разных правил: по организации, по типу чата, по меткам «юридически значимая переписка» и т.д.

### Что пошло не так

На одном из ранних прогонов (на ограниченном окружении):

- мы не до конца учли сочетания правил;
- часть сообщений в одном из сценариев попала в «кандидаты на удаление» раньше нужного срока.

Это обнаружилось по логам и дополнительным проверкам, но это был хороший звоночок.

### Как исправили и что поменяли

После этого случая мы пересмотрели подход:

1. **Ввели режим dry-run** для таких задач: сначала задание только формирует отчёт «что бы я удалил», но фактически не трогает данные.
    
2. Для особо важных политик — сделали **двухфазную схему**:
    
    - сначала формируется список кандидатов и показывается администраторам клиента,
    - только после их явного подтверждения запускается реальное удаление.

3. Существенно расширили тестирование:
    
    - добавили автотесты на комбинации разных правил;
    - прописали чек-лист для ручных тестов именно на пересечение политик.

Этот кейс хорошо показал, что:

- в B2B-продукте с юридически чувствительными данными **нельзя опираться только на “кажется, логика правильная”**;
- для таких операций нужны дополнительные ступени проверки и «страховочные» режимы.

---

## 6. Роль в команде и что я оттуда вынес

Если суммировать мою роль в Яндексе:

- я был **Go-бэкенд-разработчиком**, который вёл задачи по core-сервисам мессенджера;
- участвовал в **проектировании API и потоков данных** между сервисами мессенджера и остальной экосистемой Яндекс 360;
- активно участвовал в **код-ревью и онбординге новых разработчиков**;
- входил в **on-call-ротацию**, работал с мониторингом, логами, участвовал в postmortem-разборах.

С точки зрения опыта:

- Яндекс дал хорошее понимание, как строить **highload B2B-сервисы** с многотенантной архитектурой;
- как работать в экосистеме, где много соседних команд и платформенных сервисов, и при этом держать свой продукт в порядке;
- и как сочетать продуктовую работу (фичи, интеграции) с эксплуатацией (надёжность, метрики, алерты).





# Кратко для HR

«Я пишу на Go уже больше пяти лет. Началось всё во времена ковида: тогда я попал на стажировку в Яндекс, в направление Яндекс 360. Это большое семейство корпоративных сервисов — Телемост, Мессенджер, Календарь, Почта и другие инструменты. Меня распределили в команду Мессенджера.

После стажировки меня взяли в штат, и за пару лет я вырос до уровня мидла. В Яндекс.Мессенджере я занимался развитием ключевых частей продукта: улучшал работу с диалогами и историей сообщений, участвовал в развитии системы уведомлений, а также в интеграциях с другими продуктами Яндекса — почтой, календарём, документами.

Работа была интересной, потому что сервис используется крупными компаниями, нагрузка высокая, требований много, и каждое изменение нужно продумывать заранее. Мы постоянно улучшали производительность, работали с базами данных, кэшем, оптимизировали задержки, делали новые фичи по запросу клиентов. Параллельно я участвовал в on-call дежурствах, разбирал инциденты и улучшал наблюдаемость в сервисах. Это дало мне strong фундамент в качестве, надёжности и ответственности.

Через два с половиной года я понял, что хочу большего влияния на архитектуру и возможность строить продукт практически с нуля. В этот момент меня позвал друг в молодую, но быстрорастущую финтех-компанию StormTrade, на новый проект — Upscale. Там как раз начиналась активная фаза развития продукта.

Когда я пришёл, у ребят был MVP в виде монолита — одно большое приложение. Для раннего этапа это нормально, но когда начали расти количество трейдеров и операций, стало понятно, что такую систему нужно перерабатывать. Мы разделили продукт на несколько микросервисов, чтобы каждый компонент можно было развивать, масштабировать и обновлять независимо.

Я активно участвовал в проектировании архитектуры, переносе функциональности из монолита в отдельные сервисы, в настройке обмена данными между ними. Мы строили торговое ядро, сервисы для уведомлений, реферальную систему, аналитику по трейдингу, работу с документами и сертификатами, разные маркетинговые механики. Параллельно внедряли метрики, мониторинг, улучшали производительность, оптимизировали базу данных, шардировали данные, делали работу системы стабильной даже при высокой нагрузке.

В Upscale у меня было больше ответственности: я участвовал в принятии архитектурных решений, помогал коллегам, занимался код-ревью, участвовал в on-call ротации и в целом помогал выстраивать инженерную культуру.

Команда Upscale: 6 backend-разработчиков (Go), 2 frontend, 2 QA, продуктовый менеджер.





# ✅ 1. **Почему Go, а не Java / Python / C# / C++?**

**Вариант ответа №1 — универсальный (для HR и тимлидов):**

> «Выбрал Go, потому что мне нравится сочетание простоты языка, прозрачной модели конкурентности и скорости разработки. Go хорошо подходит для сервисов, где важны надёжность, предсказуемость SLA и работа под нагрузкой.  
> При этом на Go можно писать быстро и без перегруза абстракциями — то, чего мне часто не хватало в Java или C#.
> 
> Python мне нравится, но для высоконагруженного бэкенда он не всегда даёт нужные гарантии по производительности. C++ слишком дорог в разработке и сопровождении.
> 
> Go для меня идеальный баланс: простой, быстрый, предсказуемый, отлично подходит для микросервисов, сетевых систем и event-driven архитектуры.»

**Вариант №2 — более технический (если спрашивает архитектор):**

> «Я выбрал Go из-за сочетания:  
> – простоты конкурентности (goroutines, channels, sync primitives)  
> – низкого overhead у рантайма  
> – возможности писать сервисы с минимальными накладными расходами  
> – малого порога входа в команду  
> – стабильности ABI/инструментов и чёткой backward-compatibility
> 
> Для продуктов уровня highload Go даёт очень хорошее соотношение “скорость разработки → производительность → поддерживаемость”.»

---

# ✅ 2. **Как я собирал метрики?**

**Коротко и по делу:**

> «Основной стек — Prometheus + Grafana.  
> Я всегда начинаю с базовых четырёх golden signals: latency, throughput, errors, saturation.
> 
> Добавлял:  
> – метрики обработки событий (например, backlog Kafka),  
> – бизнес-метрики (количество ордеров/запросов/диалогов),  
> – внутренние метрики шардов или воркеров.
> 
> В Go использовал прометеевские клиентские библиотеки: Histogram, Summary для latency, Counter для ошибок, Gauge для состояния.
> 
> Обязательно делал дашборды под типичные сценарии: деградация, рост нагрузки, длинные очереди, высокий p95.
> 
> Сначала собираю метрики → потом по ним настраиваю алерты → потом оптимизирую сервис на основе реальных данных.»

---

# ✅ 3. **Как анализировал простаивание сервиса?**

Это очень частый вопрос в highload-командах. Вот идеальный ответ:

> «Простаивание — это скорее не “ничего не делает”, а “не делает то, что должен”.
> 
> Я проходил по стандартной схеме:
> 
> 1. **Метрики** — смотрю на throughput, latency, utilisation CPU/RAM, backlog.
>     
> 2. **Профилирование** — pprof: goroutine dump, CPU profile, heap profile.
>     
> 3. **Очереди/внешние зависимости** — если есть Kafka/RabbitMQ/DB, изучаю lag или slow queries.
>     
> 4. **Логи** — correlation ID, доля retry, таймауты.
>     
> 5. **Внутренние блокировки** — mutex contention, goroutine leaks, проблемы с каналами.
>     
> 
> В большинстве случаев простаивание оказывалось следствием:  
> – блокировок в БД,  
> – зависания внешнего сервиса,  
> – роста лагов в Kafka,  
> – неудачной конфигурации пулов,  
> – “скрытых” бесконечных retry.
> 
> Я всегда смотрю на путь запроса целиком — от входа в API до самой глубокой зависимости.»

Это звучит как опытный Senior.

---

# ✅ 4. **Как я подходил к разработке новых сервисов?**

Вот идеальный ответ — хороший баланс инженерности и адекватности.

> «Когда я начинаю новый сервис, у меня есть несколько шагов, которые я никогда не пропускаю:
> 
> **1) Понимание домена и границ ответственности**  
> – что именно должен делать сервис;  
> – какие данные хранит;  
> – какие гарантии должен обеспечивать.
> 
> **2) Контракты API / события**  
> – определяю модель взаимодействия: REST/gRPC/Kafka;  
> – формирую контракты;  
> – учитываю backward-compatibility.
> 
> **3) Структура данных и хранилище**  
> – какие таблицы/индексы нужны;  
> – какие запросы будут частыми;  
> – какие паттерны согласованности требуются.
> 
> **4) Наблюдаемость**  
> – продумываю метрики сразу;  
> – что будем алертить;  
> – как будем профилировать.
> 
> **5) Тестируемость**  
> – unit + integration;  
> – фиксация поведения через контрактные тесты.
> 
> **6) Надёжность**  
> – retry/backoff,  
> – идемпотентность,  
> – дедубликация событий,  
> – graceful shutdown.
> 
> **7) Чистота и простота**  
> Я избегаю ненужной сложности на старте: чем меньше магии, тем проще поддерживать.
> 
> После этого сервис обычно получается предсказуемым, прозрачным и легко поддерживаемым в долгую.»

![[Pasted image 20251204135938.png]]